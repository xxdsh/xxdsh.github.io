

<!DOCTYPE html>
<html lang="en" color-mode=light>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>第十二章 计算性能 - XXDSHZJ</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="google" content="notranslate" />
  
  <meta name="description" content="计算性能影响计算性能的主要因素：命令式编程、符号编程、...">
  <meta name="author" content="Mengyuan Chen">
  <link rel="icon" href="/images/icons/favicon-16x16.png" type="image/png" sizes="16x16">
  <link rel="icon" href="/images/icons/favicon-32x32.png" type="image/png" sizes="32x32">
  <link rel="apple-touch-icon" href="/images/icons/apple-touch-icon.png" sizes="180x180">
  <meta rel="mask-icon" href="/images/icons/stun-logo.svg" color="#333333">
  
    <meta rel="msapplication-TileImage" content="/images/icons/favicon-144x144.png">
    <meta rel="msapplication-TileColor" content="#000000">
  

  
<link rel="stylesheet" href="/css/style.css">


  
    
<link rel="stylesheet" href="//at.alicdn.com/t/font_1445822_r673sha78lq.css">

  

  
    
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css">

  

  
    
      
        
        
<link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.18.1/styles/xcode.min.css" name="highlight-style" mode="light">

      
        
        
<link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.18.1/styles/solarized-dark.min.css" name="highlight-style" mode="dark">

      
  

  <script>
    var CONFIG = window.CONFIG || {};
    var ZHAOO = window.ZHAOO || {};
    CONFIG = {
      isHome: false,
      fancybox: true,
      pjax: false,
      lazyload: {
        enable: true,
        only_post: 'false',
        loading: '[object Object]'
      },
      donate: {
        enable: true,
        alipay: 'https://sm.ms/image/Y6TiL7UgNHm2RSl',
        wechat: 'https://sm.ms/image/aklIG9KSHPFcV8n'
      },
      galleries: {
        enable: true
      },
      fab: {
        enable: true,
        always_show: false
      },
      carrier: {
        enable: true
      },
      daovoice: {
        enable: false
      },
      preview: {
        background: {
          default: '',
          api: ''
        },
        motto: {
          default: '我在开了灯的床头下，想问问自己的心啊。',
          typing: true,
          api: 'https://v2.jinrishici.com/one.json',
          data_contents: '["data","content"]'
        },
      },
      qrcode: {
        enable: true,
        type: 'url',
        image: 'https://pic.izhaoo.com/weapp-code.jpg',
      },
      toc: {
        enable: true
      },
      scrollbar: {
        type: 'default'
      },
      notification: {
        enable: false,
        delay: 4500,
        list: '',
        page_white_list: '',
        page_black_list: ''
      },
      search: {
        enable: true,
        path: '/search.xml'
      }
    }
  </script>

  

  

<meta name="generator" content="Hexo 5.4.0"></head>

<body class="lock-screen">
  <div class="loading"></div>
  
    


  <nav class="navbar">
    <div class="left">
      
        <i class="iconfont iconhome j-navbar-back-home"></i>
      
      
        <i class="iconfont iconqrcode j-navbar-qrcode"></i>
      
      
        <i class="iconfont iconmoono" id="color-toggle" color-toggle="light"></i>
      
      
        <i class="iconfont iconsearch j-navbar-search"></i>
      
    </div>
    <div class="center">第十二章 计算性能</div>
    <div class="right">
      <i class="iconfont iconmenu j-navbar-menu"></i>
    </div>
    
      <div id="qrcode-navbar"></div>
    
  </nav>

  
  

<nav class="menu">
  <div class="menu-container">
    <div class="menu-close">
      <i class="iconfont iconbaseline-close-px"></i>
    </div>
    <ul class="menu-content"><li class="menu-item">
        <a href="/ " class="underline "> 首页</a>
      </li><li class="menu-item">
        <a href="/galleries/ " class="underline "> 相册</a>
      </li><li class="menu-item">
        <a href="/archives/ " class="underline "> 归档</a>
      </li><li class="menu-item">
        <a href="/tags/ " class="underline "> 标签</a>
      </li><li class="menu-item">
        <a href="/categories/ " class="underline "> 分类</a>
      </li><li class="menu-item">
        <a href="/about/ " class="underline "> 关于</a>
      </li></ul>
    
      <div class="menu-copyright"><p>Powered by <a target="_blank" href="https://hexo.io">Hexo</a>  |  Cure The World </p></div>
    
  </div>
</nav>
  <main id="main">
  <div class="article-wrap">
    <div class="row container">
      <div class="col-xl-3"></div>
      <div class="col-xl-6"><article class="article">
  <div class="wrap">
    <section class="head">
  <img   class="lazyload" data-original="/images/theme/post-image.jpg" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="  draggable="false">
  <div class="head-mask">
    <h1 class="head-title">第十二章 计算性能</h1>
    <div class="head-info">
      <span class="post-info-item"><i class="iconfont iconcalendar"></i>October 06, 2021</span>
      
      <span class="post-info-item"><i class="iconfont iconfont-size"></i>14618</span>
    </div>
  </div>
</section>
    <section class="main">
      <section class="content">
        <h1 id="计算性能"><a href="#计算性能" class="headerlink" title="计算性能"></a>计算性能</h1><p>影响计算性能的主要因素：命令式编程、符号编程、异步计算、自动并行和多GPU计算。</p>
<h2 id="12-1-编译器和解释器"><a href="#12-1-编译器和解释器" class="headerlink" title="12.1 编译器和解释器"></a>12.1 编译器和解释器</h2><p><em>命令式编程</em>（imperative programming）<br>Python是一种<em>解释型语言</em>（interpreted language）</p>
<h3 id="12-1-1-符号式编程"><a href="#12-1-1-符号式编程" class="headerlink" title="12.1.1 符号式编程"></a>12.1.1 符号式编程</h3><p><em>符号式编程</em>（symbolic programming），即代码通常只在完全定义了过程之后才执行计算。</p>
<ol>
<li>定义计算流程。</li>
<li>将流程编译成可执行的程序。</li>
<li>给定输入，调用编译好的程序执行。<br>命令式（解释型）编程和符号式编程的区别如下：</li>
</ol>
<ul>
<li>命令式编程更容易使用。在 Python 中，命令式编程的大部分代码都是简单易懂的。命令式编程也更容易调试，这是因为无论是获取和打印所有的中间变量值，或者使用 Python 的内置调试工具都更加简单。</li>
<li>符号式编程运行效率更高，更易于移植。符号式编程更容易在编译期间优化代码，同时还能够将程序移植到与 Python 无关的格式中，从而允许程序在非 Python 环境中运行，避免了任何潜在的与 Python 解释器相关的性能问题。<h3 id="12-1-2-混合式编程"><a href="#12-1-2-混合式编程" class="headerlink" title="12.1.2 混合式编程"></a>12.1.2 混合式编程</h3><h3 id="12-1-3-Sequential的混合式编程"><a href="#12-1-3-Sequential的混合式编程" class="headerlink" title="12.1.3 Sequential的混合式编程"></a>12.1.3 <code>Sequential</code>的混合式编程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><br><span class="hljs-comment"># 生产网络的工厂模式</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_net</span>():</span><br>    net = nn.Sequential(nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">256</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">128</span>, <span class="hljs-number">2</span>))<br>    <span class="hljs-keyword">return</span> net<br><br>x = torch.randn(size=(<span class="hljs-number">1</span>, <span class="hljs-number">512</span>))<br>net = get_net()<br>net(x)<br></code></pre></td></tr></table></figure>
<code>torch.jit.script</code> 函数来转换模型<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">net = torch<span class="hljs-selector-class">.jit</span><span class="hljs-selector-class">.script</span>(net)<br><span class="hljs-function"><span class="hljs-title">net</span><span class="hljs-params">(x)</span></span><br></code></pre></td></tr></table></figure></li>
</ul>
<h4 id="通过混合式编程加速"><a href="#通过混合式编程加速" class="headerlink" title="通过混合式编程加速"></a>通过混合式编程加速</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Benchmark</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, description=<span class="hljs-string">&#x27;Done&#x27;</span></span>):</span><br>        self.description = description<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__enter__</span>(<span class="hljs-params">self</span>):</span><br>        self.timer = d2l.Timer()<br>        <span class="hljs-keyword">return</span> self<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__exit__</span>(<span class="hljs-params">self, *args</span>):</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;self.description&#125;</span>: <span class="hljs-subst">&#123;self.timer.stop():<span class="hljs-number">.4</span>f&#125;</span> sec&#x27;</span>)<br>net = get_net()<br><span class="hljs-keyword">with</span> Benchmark(<span class="hljs-string">&#x27;无torchscript&#x27;</span>):<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>): net(x)<br><br>net = torch.jit.script(net)<br><span class="hljs-keyword">with</span> Benchmark(<span class="hljs-string">&#x27;有torchscript&#x27;</span>):<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>): net(x)<br></code></pre></td></tr></table></figure>
<h4 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h4><p>可以将模型及其参数序列化（保存）到磁盘。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">net.save(<span class="hljs-string">&#x27;my_mlp&#x27;</span>)<br>!ls -lh my_mlp*<br></code></pre></td></tr></table></figure>
<h3 id="12-1-4-小结"><a href="#12-1-4-小结" class="headerlink" title="12.1.4 小结"></a>12.1.4 小结</h3><ul>
<li>命令式编程使得新模型的设计变得容易，因为可以依据控制流编写代码，并拥有相对成熟的 Python 软件生态。</li>
<li>符号式编程要求先定义并且编译程序，然后再执行程序，其好处是提高了计算性能。</li>
</ul>
<h2 id="12-3-异步计算"><a href="#12-3-异步计算" class="headerlink" title="12.3 异步计算"></a>12.3 异步计算</h2><p>多个 CPU 核、多个 GPU、多个处理单元组成。<br>通常每个 CPU 核有多个线程，每个设备通常有多个 GPU ，每个 GPU 有多个处理单元。<br>MXNet 和 TensorFlow 之类则采用了一种 <em>异步编程</em>（asynchronous programming）模型来提高性能，而 PyTorch 则使用了 Python 自己的调度器来实现不同的性能权衡。</p>
<p>了解异步编程是如何工作的，通过主动地减少计算需求和相互依赖，有助于开发更高效的程序，能够减少内存开销并提高处理器利用率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> subprocess<br><span class="hljs-keyword">import</span> numpy<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br></code></pre></td></tr></table></figure>

<h3 id="12-2-1-通过后端异步处理"><a href="#12-2-1-通过后端异步处理" class="headerlink" title="12.2.1 通过后端异步处理"></a>12.2.1 通过后端异步处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># GPU 计算热身</span><br>device = d2l.try_gpu()<br>a = torch.randn(size=(<span class="hljs-number">1000</span>, <span class="hljs-number">1000</span>), device=device)<br>b = torch.mm(a, a)<br><br><span class="hljs-keyword">with</span> d2l.Benchmark(<span class="hljs-string">&#x27;numpy&#x27;</span>):<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>        a = numpy.random.normal(size=(<span class="hljs-number">1000</span>, <span class="hljs-number">1000</span>))<br>        b = numpy.dot(a, a)<br><br><span class="hljs-keyword">with</span> d2l.Benchmark(<span class="hljs-string">&#x27;torch&#x27;</span>):<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>        a = torch.randn(size=(<span class="hljs-number">1000</span>, <span class="hljs-number">1000</span>), device=device)<br>        b = torch.mm(a, a)<br></code></pre></td></tr></table></figure>
<p>默认情况下，GPU 操作在 PyTorch 中是异步的。强制 PyTorch 在返回之前完成所有计算，这种强制说明了之前发生的情况：计算是由后端执行，而前端将控制权返回给了 Python。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> d2l.Benchmark():<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>        a = torch.randn(size=(<span class="hljs-number">1000</span>, <span class="hljs-number">1000</span>), device=device)<br>        b = torch.mm(a, a)<br>    torch.cuda.synchronize(device)<br></code></pre></td></tr></table></figure>
<p>依赖关系图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.ones((<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), device=device)<br>y = torch.ones((<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), device=device)<br>z = x * y + <span class="hljs-number">2</span><br>z<br></code></pre></td></tr></table></figure>
<h3 id="12-2-2-障碍器与阻塞器"><a href="#12-2-2-障碍器与阻塞器" class="headerlink" title="12.2.2 障碍器与阻塞器"></a>12.2.2 障碍器与阻塞器</h3><h3 id="12-2-3-改进计算"><a href="#12-2-3-改进计算" class="headerlink" title="12.2.3 改进计算"></a>12.2.3 改进计算</h3><h3 id="12-2-4-小结"><a href="#12-2-4-小结" class="headerlink" title="12.2.4 小结"></a>12.2.4 小结</h3><ul>
<li>深度学习框架可以将 Python 前端的控制与后端的执行解耦，使得命令可以快速地异步插入后端、并行执行。</li>
<li>异步产生了一个相当灵活的前端，但请注意：过度填充任务队列可能会导致内存消耗过多。建议对每个小批量进行同步，以保持前端和后端大致同步。</li>
<li>芯片供应商提供了复杂的性能分析工具，以获得对深度学习效率更精确的洞察。</li>
</ul>
<h2 id="12-3-自动并行"><a href="#12-3-自动并行" class="headerlink" title="12.3 自动并行"></a>12.3 自动并行</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br></code></pre></td></tr></table></figure>

<h3 id="12-3-1-基于GPU的并行计算"><a href="#12-3-1-基于GPU的并行计算" class="headerlink" title="12.3.1 基于GPU的并行计算"></a>12.3.1 基于GPU的并行计算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">devices = d2l.try_all_gpus()<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">return</span> [x.mm(x) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">50</span>)]<br><br>x_gpu1 = torch.rand(size=(<span class="hljs-number">4000</span>, <span class="hljs-number">4000</span>), device=devices[<span class="hljs-number">0</span>])<br>x_gpu2 = torch.rand(size=(<span class="hljs-number">4000</span>, <span class="hljs-number">4000</span>), device=devices[<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure>
<p>预热设备（对设备执行一次传递）来确保缓存的作用不影响最终的结果。<code>torch.cuda.synchronize()</code> 函数将会等待一个 CUDA 设备上的所有流中的所有核心的计算完成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">run(x_gpu1)<br>run(x_gpu2)  <span class="hljs-comment"># 预热设备</span><br>torch.cuda.synchronize(devices[<span class="hljs-number">0</span>])<br>torch.cuda.synchronize(devices[<span class="hljs-number">1</span>])<br><br><span class="hljs-keyword">with</span> d2l.Benchmark(<span class="hljs-string">&#x27;GPU1 time&#x27;</span>):<br>    run(x_gpu1)<br>    torch.cuda.synchronize(devices[<span class="hljs-number">0</span>])<br><br><span class="hljs-keyword">with</span> d2l.Benchmark(<span class="hljs-string">&#x27;GPU2 time&#x27;</span>):<br>    run(x_gpu2)<br>    torch.cuda.synchronize(devices[<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure>
<p>删除两个任务之间的 <code>synchronize</code> 语句，系统就可以在两个设备上自动实现并行计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> d2l.Benchmark(<span class="hljs-string">&#x27;GPU1 &amp; GPU2&#x27;</span>):<br>    run(x_gpu1)<br>    run(x_gpu2)<br>    torch.cuda.synchronize()<br></code></pre></td></tr></table></figure>
<h3 id="12-3-2-并行计算与通信"><a href="#12-3-2-并行计算与通信" class="headerlink" title="12.3.2 并行计算与通信"></a>12.3.2 并行计算与通信</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">copy_to_cpu</span>(<span class="hljs-params">x, non_blocking=<span class="hljs-literal">False</span></span>):</span><br>    <span class="hljs-keyword">return</span> [y.to(<span class="hljs-string">&#x27;cpu&#x27;</span>, non_blocking=non_blocking) <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> x]<br><br><span class="hljs-keyword">with</span> d2l.Benchmark(<span class="hljs-string">&#x27;在GPU1上运行&#x27;</span>):<br>    y = run(x_gpu1)<br>    torch.cuda.synchronize()<br><br><span class="hljs-keyword">with</span> d2l.Benchmark(<span class="hljs-string">&#x27;复制到CPU&#x27;</span>):<br>    y_cpu = copy_to_cpu(y)<br>    torch.cuda.synchronize()<br></code></pre></td></tr></table></figure>
<p>在 PyTorch 中，<code>to() </code>和 <code>copy_()</code> 等函数都允许显式的 <code>non_blocking</code> 参数，这允许在不需要同步时调用方可以绕过同步。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> d2l.Benchmark(<span class="hljs-string">&#x27;在GPU1上运行并复制到CPU&#x27;</span>):<br>    y = run(x_gpu1)<br>    y_cpu = copy_to_cpu(y, <span class="hljs-literal">True</span>)<br>    torch.cuda.synchronize()<br></code></pre></td></tr></table></figure>
<p>两个操作所需的总时间少于它们各部分操作所需时间的总和。<br>与并行计算的区别是通信操作使用的资源：CPU 和 GPU 之间的总线。</p>
<h3 id="12-3-3-小结"><a href="#12-3-3-小结" class="headerlink" title="12.3.3 小结"></a>12.3.3 小结</h3><ul>
<li>现代系统拥有多种设备，如多个 GPU 和多个 CPU，还可以并行地、异步地使用它们。</li>
<li>现代系统还拥有各种通信资源，如PCI Express、存储（通常是固态硬盘或网络存储）和网络带宽，为了达到最高效率可以并行使用它们。</li>
<li>后端可以通过自动化地并行计算和通信来提高性能。</li>
</ul>
<h2 id="12-4-硬件"><a href="#12-4-硬件" class="headerlink" title="12.4 硬件"></a>12.4 硬件</h2><h3 id="12-4-1-计算机"><a href="#12-4-1-计算机" class="headerlink" title="12.4.1 计算机"></a>12.4.1 计算机</h3><p>一台具有相当数量的内存、计算资源、某种形式的加速器（如一个或者多个 GPU）的计算机。</p>
<ul>
<li>一个处理器（也被称为 CPU），它除了能够运行操作系统和许多其他功能之外，还能够执行给它的程序，通常由 <svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.162ex" height="2.176ex" style="vertical-align: -0.338ex;" viewBox="0 -791.3 500.5 936.9" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">8</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMAIN-38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMAIN-38" x="0" y="0"></use>
</g>
</svg> 个或更多个核心组成。</li>
<li>内存（RAM）用于存储和检索计算结果，如权重向量和激活参数，以及训练数据。</li>
<li>一个或多个以太网连接，速度从 1 GB/s 到 100 GB/s 不等。在高端服务器上可能用到更高级的互连。</li>
<li>高速扩展总线（PCIe）用于系统连接一个或多个 GPU 。服务器最多有 <svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.162ex" height="2.176ex" style="vertical-align: -0.338ex;" viewBox="0 -791.3 500.5 936.9" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">8</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMAIN-38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMAIN-38" x="0" y="0"></use>
</g>
</svg> 个加速卡，通常以更高级的拓扑方式连接，而桌面系统则有 <svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.162ex" height="2.176ex" style="vertical-align: -0.338ex;" viewBox="0 -791.3 500.5 936.9" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">1</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMAIN-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMAIN-31" x="0" y="0"></use>
</g>
</svg> 个或 <svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.162ex" height="2.176ex" style="vertical-align: -0.338ex;" viewBox="0 -791.3 500.5 936.9" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">2</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMAIN-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMAIN-32" x="0" y="0"></use>
</g>
</svg> 个加速卡，具体取决于用户的预算和电源负载的大小。</li>
<li>持久性存储设备，如磁盘驱动器、固态驱动器，在许多情况下使用高速扩展总线连接。它为系统需要的训练数据和中间检查点需要的存储提供了足够的传输速度。<h3 id="12-4-2-内存"><a href="#12-4-2-内存" class="headerlink" title="12.4.2 内存"></a>12.4.2 内存</h3>最基本的内存主要用于存储需要随时访问的数据。</li>
</ul>
<p>GPU内存的带宽要求甚至更高，因为它们的处理单元比CPU多得多。总的来说，解决这些问题有两种选择。首先是使内存总线变得更宽。其次，GPU使用特定的高性能内存。</p>
<ul>
<li>提升空间和时间的内存本地性</li>
<li>时间：重用数据使得保持它们在缓存里</li>
<li>空间：按序读写数据使得可以预读取<h3 id="12-4-3-存储器"><a href="#12-4-3-存储器" class="headerlink" title="12.4.3 存储器"></a>12.4.3 存储器</h3>RAM关键特性是 <em>带宽</em>（bandwidth）和 <em>延迟</em>（latency）。<h4 id="硬盘驱动器"><a href="#硬盘驱动器" class="headerlink" title="硬盘驱动器"></a>硬盘驱动器</h4></li>
<li>硬盘驱动器*（Hard disk drives，HDDs）<br>硬盘优点：相对便宜；缺点：典型的灾难性故障模式和相对较高的读取延迟。<h4 id="固态驱动器"><a href="#固态驱动器" class="headerlink" title="固态驱动器"></a>固态驱动器</h4>固态驱动器（Solid state drives，SSD）使用闪存持久地存储信息。<h4 id="云存储"><a href="#云存储" class="headerlink" title="云存储"></a>云存储</h4></li>
</ul>
<h3 id="12-4-4-CPU"><a href="#12-4-4-CPU" class="headerlink" title="12.4.4 CPU"></a>12.4.4 CPU</h3><p>中央处理器（CPU）是任何计算机的核心。它们由许多关键组件组成：<em>处理器核心</em>（processor cores）用于执行机器代码的、<em>总线</em>（bus）用于连接不同组件（注意，总线会因为处理器型号、各代产品和供应商之间的特定拓扑结构有明显不同）和<em>缓存</em>（caches）相比主内存实现更高的读取带宽和更低的延迟内存访问。<br><em>向量处理单元</em>（vector processing units）</p>
<h4 id="微体系结构"><a href="#微体系结构" class="headerlink" title="微体系结构"></a>微体系结构</h4><h4 id="矢量化"><a href="#矢量化" class="headerlink" title="矢量化"></a>矢量化</h4><p>一个常见的功能是它们能够执行单指令多数据（single instruction multiple data，SIMD）操作</p>
<h4 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h4><ul>
<li><strong>寄存器</strong>，严格来说不是缓存的一部分，用于帮助组织指令。</li>
<li><strong>一级缓存</strong> 是应对高内存带宽要求的第一道防线。</li>
<li><strong>二级缓存</strong> 是下一站。</li>
<li><strong>三级缓存</strong> 在多个核之间共享，并且可以非常大。</li>
</ul>
<h3 id="12-4-5-GPU和其他加速卡"><a href="#12-4-5-GPU和其他加速卡" class="headerlink" title="12.4.5 GPU和其他加速卡"></a>12.4.5 GPU和其他加速卡</h3><p><em>张量核</em>（tensor cores）<br>TPU 添加了用于快速矩阵乘法的脉动阵列 </p>
<h3 id="12-4-6-网络和总线"><a href="#12-4-6-网络和总线" class="headerlink" title="12.4.6 网络和总线"></a>12.4.6 网络和总线</h3><p>设计参数：带宽、成本、距离和灵活性。</p>
<ul>
<li><strong>PCIe</strong>，一种专用总线，用于每个通道点到点连接的高带宽需求（在 <svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.325ex" height="2.176ex" style="vertical-align: -0.338ex;" viewBox="0 -791.3 1001 936.9" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">16</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMAIN-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path>
<path stroke-width="1" id="E1-MJMAIN-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMAIN-31"></use>
 <use xlink:href="#E1-MJMAIN-36" x="500" y="0"></use>
</g>
</svg> 通道插槽中的 PCIe 4.0 上高达 32 GB/s），延迟时间为个位数的微秒（5 μs）。</li>
<li><strong>以太网</strong>，连接计算机最常用的方式。</li>
<li><strong>交换机</strong>，一种连接多个设备的方式，该连接方式下的任何一对设备都可以同时执行（通常是全带宽）点对点连接。</li>
<li><strong>NVLink</strong>，是 PCIe 的替代品，适用于非常高带宽的互连。<h3 id="12-4-7-更多延迟"><a href="#12-4-7-更多延迟" class="headerlink" title="12.4.7 更多延迟"></a>12.4.7 更多延迟</h3><h3 id="12-4-8-小结"><a href="#12-4-8-小结" class="headerlink" title="12.4.8 小结"></a>12.4.8 小结</h3></li>
<li>设备有运行开销。因此，数据传输要争取量大次少而不是量少次多。这适用于 RAM、固态驱动器、网络和 GPU。</li>
<li>矢量化是性能的关键。确保充分了解你的加速器的特定功能。</li>
<li>在训练过程中数据类型过小导致的数值溢出可能是个问题（在推理过程中则影响不大）。</li>
<li>数据混叠现象会导致严重的性能退化。<svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.325ex" height="2.176ex" style="vertical-align: -0.338ex;" viewBox="0 -791.3 1001 936.9" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">64</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMAIN-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path>
<path stroke-width="1" id="E1-MJMAIN-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMAIN-36"></use>
 <use xlink:href="#E1-MJMAIN-34" x="500" y="0"></use>
</g>
</svg> 位 CPU 应该按照 <svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.325ex" height="2.176ex" style="vertical-align: -0.338ex;" viewBox="0 -791.3 1001 936.9" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">64</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMAIN-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path>
<path stroke-width="1" id="E1-MJMAIN-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMAIN-36"></use>
 <use xlink:href="#E1-MJMAIN-34" x="500" y="0"></use>
</g>
</svg> 位边界进行内存对齐。在 GPU 上建议保持卷积大小对齐，例如：与张量核对齐。</li>
<li>将算法与硬件相匹配（例如，内存占用和带宽）。将命中参数装入缓存后，可以实现很大数量级的加速比。</li>
<li>在验证实验结果之前，建议先在纸上勾勒出新算法的性能。关注的原因是数量级及以上的差异。</li>
<li>使用调试器跟踪调试寻找性能的瓶颈。</li>
<li>训练硬件和推理硬件在性能和价格方面有不同的优点。</li>
</ul>
<h2 id="12-5-多GPU训练"><a href="#12-5-多GPU训练" class="headerlink" title="12.5 多GPU训练"></a>12.5 多GPU训练</h2><h3 id="12-5-1-问题拆分"><a href="#12-5-1-问题拆分" class="headerlink" title="12.5.1 问题拆分"></a>12.5.1 问题拆分</h3><p>以一种方式对训练进行拆分，为实现良好的加速比，还能同时受益于简单且可重复的设计选择。<br>第一种方法，在多个 GPU 之间拆分网络。网络并行<br>第二种方法，拆分层内的工作。分层并行<br>需要大量的同步或<em>屏障操作</em>(barrier operations)<br>最后一种方法，跨多个 GPU 对数据进行拆分。数据并行</p>
<h3 id="12-5-2-数据并行性"><a href="#12-5-2-数据并行性" class="headerlink" title="12.5.2 数据并行性"></a>12.5.2 数据并行性</h3><ul>
<li>在任何一次训练迭代中，给定的随机的小批量样本都将被分成 <svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.211ex" height="2.176ex" style="vertical-align: -0.338ex;" viewBox="0 -791.3 521.5 936.9" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">k</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMATHI-6B" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMATHI-6B" x="0" y="0"></use>
</g>
</svg> 个部分，并均匀地分配到 GPU 上。</li>
<li>每个 GPU 根据分配给它的小批量子集，计算模型参数的损失和梯度。</li>
<li>将 <svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.211ex" height="2.176ex" style="vertical-align: -0.338ex;" viewBox="0 -791.3 521.5 936.9" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">k</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMATHI-6B" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMATHI-6B" x="0" y="0"></use>
</g>
</svg> 个 GPU 中的局部梯度聚合，以获得当前小批量的随机梯度。</li>
<li>聚合梯度被重新分发到每个 GPU 中。</li>
<li>每个 GPU 使用这个小批量随机梯度，来更新它所维护的完整的模型参数集。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">%matplotlib inline<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="12-5-3-简单网络"><a href="#12-5-3-简单网络" class="headerlink" title="12.5.3 简单网络"></a>12.5.3 简单网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 初始化模型参数</span><br>scale = <span class="hljs-number">0.01</span><br>W1 = torch.randn(size=(<span class="hljs-number">20</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>)) * scale<br>b1 = torch.zeros(<span class="hljs-number">20</span>)<br>W2 = torch.randn(size=(<span class="hljs-number">50</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>)) * scale<br>b2 = torch.zeros(<span class="hljs-number">50</span>)<br>W3 = torch.randn(size=(<span class="hljs-number">800</span>, <span class="hljs-number">128</span>)) * scale<br>b3 = torch.zeros(<span class="hljs-number">128</span>)<br>W4 = torch.randn(size=(<span class="hljs-number">128</span>, <span class="hljs-number">10</span>)) * scale<br>b4 = torch.zeros(<span class="hljs-number">10</span>)<br>params = [W1, b1, W2, b2, W3, b3, W4, b4]<br><br><span class="hljs-comment"># 定义模型</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">lenet</span>(<span class="hljs-params">X, params</span>):</span><br>    h1_conv = F.conv2d(<span class="hljs-built_in">input</span>=X, weight=params[<span class="hljs-number">0</span>], bias=params[<span class="hljs-number">1</span>])<br>    h1_activation = F.relu(h1_conv)<br>    h1 = F.avg_pool2d(<span class="hljs-built_in">input</span>=h1_activation, kernel_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))<br>    h2_conv = F.conv2d(<span class="hljs-built_in">input</span>=h1, weight=params[<span class="hljs-number">2</span>], bias=params[<span class="hljs-number">3</span>])<br>    h2_activation = F.relu(h2_conv)<br>    h2 = F.avg_pool2d(<span class="hljs-built_in">input</span>=h2_activation, kernel_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))<br>    h2 = h2.reshape(h2.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>)<br>    h3_linear = torch.mm(h2, params[<span class="hljs-number">4</span>]) + params[<span class="hljs-number">5</span>]<br>    h3 = F.relu(h3_linear)<br>    y_hat = torch.mm(h3, params[<span class="hljs-number">6</span>]) + params[<span class="hljs-number">7</span>]<br>    <span class="hljs-keyword">return</span> y_hat<br><br><span class="hljs-comment"># 交叉熵损失函数</span><br>loss = nn.CrossEntropyLoss(reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br></code></pre></td></tr></table></figure>

<h3 id="12-5-4-数据同步"><a href="#12-5-4-数据同步" class="headerlink" title="12.5.4 数据同步"></a>12.5.4 数据同步</h3><p>[<strong>向多个设备分发参数</strong>] 并附加梯度（<code>get_params</code>）。<br>需要跨多个设备对参数求和</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_params</span>(<span class="hljs-params">params, device</span>):</span><br>    new_params = [p.clone().to(device) <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> params]<br>    <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> new_params:<br>        p.requires_grad_()<br>    <span class="hljs-keyword">return</span> new_params<br></code></pre></td></tr></table></figure>
<p>通过将模型参数复制到一个GPU。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">new_params = get_params(params, d2l.try_gpu(<span class="hljs-number">0</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;b1 weight:&#x27;</span>, new_params[<span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;b1 grad:&#x27;</span>, new_params[<span class="hljs-number">1</span>].grad)<br></code></pre></td></tr></table></figure>
<p>[<strong><code>allreduce</code> 函数将所有向量相加，并将结果广播给所有 GPU</strong>]。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">allreduce</span>(<span class="hljs-params">data</span>):</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(data)):<br>        data[<span class="hljs-number">0</span>][:] += data[i].to(data[<span class="hljs-number">0</span>].device)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(data)):<br>        data[i] = data[<span class="hljs-number">0</span>].to(data[i].device)<br></code></pre></td></tr></table></figure>
<p>通过在不同设备上创建具有不同值的向量并聚合它们。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">data = [torch.ones((<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), device=d2l.try_gpu(i)) * (i + <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>)]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;before allreduce:\n&#x27;</span>, data[<span class="hljs-number">0</span>], <span class="hljs-string">&#x27;\n&#x27;</span>, data[<span class="hljs-number">1</span>])<br>allreduce(data)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;after allreduce:\n&#x27;</span>, data[<span class="hljs-number">0</span>], <span class="hljs-string">&#x27;\n&#x27;</span>, data[<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure>
<h3 id="12-5-5-数据分发"><a href="#12-5-5-数据分发" class="headerlink" title="12.5.5 数据分发"></a>12.5.5 数据分发</h3><p>[<strong>将一个小批量数据均匀地分布在多个 GPU 上</strong>]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">data = torch.arange(<span class="hljs-number">20</span>).reshape(<span class="hljs-number">4</span>, <span class="hljs-number">5</span>)<br>devices = [torch.device(<span class="hljs-string">&#x27;cuda:0&#x27;</span>), torch.device(<span class="hljs-string">&#x27;cuda:1&#x27;</span>)]<br>split = nn.parallel.scatter(data, devices)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;input :&#x27;</span>, data)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;load into&#x27;</span>, devices)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;output:&#x27;</span>, split)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">split_batch</span>(<span class="hljs-params">X, y, devices</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;将`X`和`y`拆分到多个设备上&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">assert</span> X.shape[<span class="hljs-number">0</span>] == y.shape[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">return</span> (nn.parallel.scatter(X, devices),<br>            nn.parallel.scatter(y, devices))<br></code></pre></td></tr></table></figure>
<h3 id="12-5-6-训练"><a href="#12-5-6-训练" class="headerlink" title="12.5.6 训练"></a>12.5.6 训练</h3><p>[<strong>在一个小批量上实现多 GPU 训练</strong>]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_batch</span>(<span class="hljs-params">X, y, device_params, devices, lr</span>):</span><br>    X_shards, y_shards = split_batch(X, y, devices)<br>    <span class="hljs-comment"># 在每个GPU上分别计算损失</span><br>    ls = [loss(lenet(X_shard, device_W), y_shard).<span class="hljs-built_in">sum</span>()<br>          <span class="hljs-keyword">for</span> X_shard, y_shard, device_W <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<br>              X_shards, y_shards, device_params)]<br>    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> ls:  <span class="hljs-comment"># 反向传播在每个GPU上分别执行</span><br>        l.backward()<br>    <span class="hljs-comment"># 将每个GPU的所有梯度相加，并将其广播到所有GPU</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(device_params[<span class="hljs-number">0</span>])):<br>            allreduce([device_params[c][i].grad <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(devices))])<br>    <span class="hljs-comment"># 在每个GPU上分别更新模型参数</span><br>    <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> device_params:<br>        d2l.sgd(param, lr, X.shape[<span class="hljs-number">0</span>]) <span class="hljs-comment"># 在这里，我们使用全尺寸的小批量</span><br></code></pre></td></tr></table></figure>
<p>[<strong>定义训练函数</strong>]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">num_gpus, batch_size, lr</span>):</span><br>    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)<br>    devices = [d2l.try_gpu(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_gpus)]<br>    <span class="hljs-comment"># 将模型参数复制到`num_gpus`个GPU</span><br>    device_params = [get_params(params, d) <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> devices]<br>    num_epochs = <span class="hljs-number">10</span><br>    animator = d2l.Animator(<span class="hljs-string">&#x27;epoch&#x27;</span>, <span class="hljs-string">&#x27;test acc&#x27;</span>, xlim=[<span class="hljs-number">1</span>, num_epochs])<br>    timer = d2l.Timer()<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        timer.start()<br>        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> train_iter:<br>            <span class="hljs-comment"># 为单个小批量执行多GPU训练</span><br>            train_batch(X, y, device_params, devices, lr)<br>            torch.cuda.synchronize()<br>        timer.stop()<br>        <span class="hljs-comment"># 在GPU 0上评估模型</span><br>        animator.add(epoch + <span class="hljs-number">1</span>, (d2l.evaluate_accuracy_gpu(<br>            <span class="hljs-keyword">lambda</span> x: lenet(x, device_params[<span class="hljs-number">0</span>]), test_iter, devices[<span class="hljs-number">0</span>]),))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;test acc: <span class="hljs-subst">&#123;animator.Y[<span class="hljs-number">0</span>][-<span class="hljs-number">1</span>]:<span class="hljs-number">.2</span>f&#125;</span>, <span class="hljs-subst">&#123;timer.avg():<span class="hljs-number">.1</span>f&#125;</span> sec/epoch &#x27;</span><br>          <span class="hljs-string">f&#x27;on <span class="hljs-subst">&#123;<span class="hljs-built_in">str</span>(devices)&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure>
<p>[<strong>在单个GPU上运行</strong>] </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train(num_gpus=<span class="hljs-number">1</span>, batch_size=<span class="hljs-number">256</span>, lr=<span class="hljs-number">0.2</span>)<br></code></pre></td></tr></table></figure>
<p>[<strong>增加为2个GPU</strong>]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train(num_gpus=<span class="hljs-number">2</span>, batch_size=<span class="hljs-number">256</span>, lr=<span class="hljs-number">0.2</span>)<br></code></pre></td></tr></table></figure>
<h3 id="12-5-7-小结"><a href="#12-5-7-小结" class="headerlink" title="12.5.7 小结"></a>12.5.7 小结</h3><ul>
<li>有多种方法可以在多个 GPU 上拆分深度网络的训练。拆分可以在层之间、跨层或跨数据上实现。前两者需要对数据传输过程进行严格编排，而最后一种则是最简单的策略。</li>
<li>数据并行训练本身是不复杂的，它通过增加有效的小批量数据量的大小提高了训练效率。</li>
<li>在数据并行中，数据需要跨多个 GPU 拆分，其中每个 GPU 执行自己的前向传播和反向传播，随后所有的梯度被聚合为一，之后聚合结果向所有的 GPU 广播。</li>
<li>小批量数据量更大时，学习率也需要稍微提高一些。</li>
</ul>
<h2 id="12-6-多GPU的简洁实现"><a href="#12-6-多GPU的简洁实现" class="headerlink" title="12.6 多GPU的简洁实现"></a>12.6 多GPU的简洁实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br></code></pre></td></tr></table></figure>

<h3 id="12-6-1-简单网络"><a href="#12-6-1-简单网络" class="headerlink" title="12.6.1 简单网络"></a>12.6.1 简单网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">resnet18</span>(<span class="hljs-params">num_classes, in_channels=<span class="hljs-number">1</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;稍加修改的 ResNet-18 模型。&quot;&quot;&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">resnet_block</span>(<span class="hljs-params">in_channels, out_channels, num_residuals,</span></span><br><span class="hljs-params"><span class="hljs-function">                     first_block=<span class="hljs-literal">False</span></span>):</span><br>        blk = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_residuals):<br>            <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> first_block:<br>                blk.append(d2l.Residual(in_channels, out_channels,<br>                                        use_1x1conv=<span class="hljs-literal">True</span>, strides=<span class="hljs-number">2</span>))<br>            <span class="hljs-keyword">else</span>:<br>                blk.append(d2l.Residual(out_channels, out_channels))<br>        <span class="hljs-keyword">return</span> nn.Sequential(*blk)<br><br>    <span class="hljs-comment"># 该模型使用了更小的卷积核、步长和填充，而且删除了最大汇聚层。</span><br>    net = nn.Sequential(<br>        nn.Conv2d(in_channels, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),<br>        nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>        nn.ReLU())<br>    net.add_module(<span class="hljs-string">&quot;resnet_block1&quot;</span>, resnet_block(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-number">2</span>, first_block=<span class="hljs-literal">True</span>))<br>    net.add_module(<span class="hljs-string">&quot;resnet_block2&quot;</span>, resnet_block(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">2</span>))<br>    net.add_module(<span class="hljs-string">&quot;resnet_block3&quot;</span>, resnet_block(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>))<br>    net.add_module(<span class="hljs-string">&quot;resnet_block4&quot;</span>, resnet_block(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">2</span>))<br>    net.add_module(<span class="hljs-string">&quot;global_avg_pool&quot;</span>, nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))<br>    net.add_module(<span class="hljs-string">&quot;fc&quot;</span>, nn.Sequential(nn.Flatten(),<br>                                       nn.Linear(<span class="hljs-number">512</span>, num_classes)))<br>    <span class="hljs-keyword">return</span> net<br></code></pre></td></tr></table></figure>
<h3 id="12-2-2-网络初始化"><a href="#12-2-2-网络初始化" class="headerlink" title="12.2.2 网络初始化"></a>12.2.2 网络初始化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">net = resnet18(<span class="hljs-number">10</span>)<br><span class="hljs-comment"># 获取GPU列表</span><br>devices = d2l.try_all_gpus()<br><span class="hljs-comment"># 我们将在训练代码实现中初始化网络</span><br></code></pre></td></tr></table></figure>

<h3 id="12-6-3-训练"><a href="#12-6-3-训练" class="headerlink" title="12.6.3 训练*"></a>12.6.3 训练*</h3><ul>
<li>需要在所有设备上初始化网络参数。</li>
<li>在数据集上迭代时，要将小批量数据分配到所有设备上。</li>
<li>跨设备并行计算损失及其梯度。</li>
<li>聚合梯度，并相应地更新参数。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">net, num_gpus, batch_size, lr</span>):</span><br>    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)<br>    devices = [d2l.try_gpu(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_gpus)]<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_weights</span>(<span class="hljs-params">m</span>):</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m) <span class="hljs-keyword">in</span> [nn.Linear, nn.Conv2d]:<br>            nn.init.normal_(m.weight, std=<span class="hljs-number">0.01</span>)<br>    net.apply(init_weights)<br>    <span class="hljs-comment"># 在多个 GPU 上设置模型</span><br>    net = nn.DataParallel(net, device_ids=devices)<br>    trainer = torch.optim.SGD(net.parameters(), lr)<br>    loss = nn.CrossEntropyLoss()<br>    timer, num_epochs = d2l.Timer(), <span class="hljs-number">10</span><br>    animator = d2l.Animator(<span class="hljs-string">&#x27;epoch&#x27;</span>, <span class="hljs-string">&#x27;test acc&#x27;</span>, xlim=[<span class="hljs-number">1</span>, num_epochs])<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        net.train()<br>        timer.start()<br>        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> train_iter:<br>            trainer.zero_grad()<br>            X, y = X.to(devices[<span class="hljs-number">0</span>]), y.to(devices[<span class="hljs-number">0</span>])<br>            l = loss(net(X), y)<br>            l.backward()<br>            trainer.step()<br>        timer.stop()<br>        animator.add(epoch + <span class="hljs-number">1</span>, (d2l.evaluate_accuracy_gpu(net, test_iter),))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;test acc: <span class="hljs-subst">&#123;animator.Y[<span class="hljs-number">0</span>][-<span class="hljs-number">1</span>]:<span class="hljs-number">.2</span>f&#125;</span>, <span class="hljs-subst">&#123;timer.avg():<span class="hljs-number">.1</span>f&#125;</span> sec/epoch &#x27;</span><br>          <span class="hljs-string">f&#x27;on <span class="hljs-subst">&#123;<span class="hljs-built_in">str</span>(devices)&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure>
[<strong>在单个GPU上训练网络</strong>]<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train(net, num_gpus=<span class="hljs-number">1</span>, batch_size=<span class="hljs-number">256</span>, lr=<span class="hljs-number">0.1</span>)<br></code></pre></td></tr></table></figure>
[<strong>使用 2 个 GPU 进行训练</strong>]<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train(net, num_gpus=<span class="hljs-number">2</span>, batch_size=<span class="hljs-number">512</span>, lr=<span class="hljs-number">0.2</span>)<br></code></pre></td></tr></table></figure>
<h3 id="12-6-4-小结"><a href="#12-6-4-小结" class="headerlink" title="12.6.4 小结"></a>12.6.4 小结</h3></li>
<li>神经网络可以在（可找到数据的）单 GPU 上进行自动评估。</li>
<li>注意每台设备上的网络需要先初始化，然后再尝试访问该设备上的参数，否则会遇到错误。</li>
<li>优化算法在多个 GPU 上自动聚合。</li>
</ul>
<h2 id="12-7-参数服务器"><a href="#12-7-参数服务器" class="headerlink" title="12.7 参数服务器"></a>12.7 参数服务器</h2><p>参数服务器的核心思想首先是在分布式隐变量模型的背景下引入的。</p>
<h3 id="12-7-1-数据并行训练"><a href="#12-7-1-数据并行训练" class="headerlink" title="12.7.1 数据并行训练"></a>12.7.1 数据并行训练</h3><h3 id="12-7-2-环同步（Ring-Synchronization）"><a href="#12-7-2-环同步（Ring-Synchronization）" class="headerlink" title="12.7.2 环同步（Ring Synchronization）"></a>12.7.2 环同步（Ring Synchronization）</h3><h3 id="12-7-3-多机训练"><a href="#12-7-3-多机训练" class="headerlink" title="12.7.3 多机训练"></a>12.7.3 多机训练</h3><p><em>同步</em>（synchronize）</p>
<h3 id="12-7-4-键值存储"><a href="#12-7-4-键值存储" class="headerlink" title="12.7.4 键值存储"></a>12.7.4 键值存储</h3><p style="text-align:center"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="23.886ex" height="5.843ex" style="vertical-align: -3.338ex;" viewBox="0 -1078.4 10284.2 2515.6" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">\mathbf{g}_{i} = \sum_{k \in \text{workers}} \sum_{j \in \text{GPUs}} \mathbf{g}_{ijk},</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMAINB-67" d="M50 300Q50 368 105 409T255 450Q328 450 376 426L388 420Q435 455 489 455Q517 455 533 441T554 414T558 389Q558 367 544 353T508 339Q484 339 471 354T458 387Q458 397 462 400Q464 401 461 400Q459 400 454 399Q429 392 427 390Q454 353 459 328Q461 315 461 300Q461 240 419 202Q364 149 248 149Q185 149 136 172Q129 158 129 148Q129 105 170 93Q176 91 263 91Q273 91 298 91T334 91T366 89T400 85T432 77T466 64Q544 22 544 -69Q544 -114 506 -145Q438 -201 287 -201Q149 -201 90 -161T30 -70Q30 -58 33 -47T42 -27T54 -13T69 -1T82 6T94 12T101 15Q66 57 66 106Q66 151 90 187L97 197L89 204Q50 243 50 300ZM485 403H492Q491 404 488 404L485 403V403ZM255 200Q279 200 295 206T319 219T331 242T335 268T336 300Q336 337 333 352T317 380Q298 399 255 399Q228 399 211 392T187 371T178 345T176 312V300V289Q176 235 194 219Q215 200 255 200ZM287 -150Q357 -150 400 -128T443 -71Q443 -65 442 -61T436 -50T420 -37T389 -27T339 -21L308 -20Q276 -20 253 -20Q190 -20 180 -20T156 -26Q130 -38 130 -69Q130 -105 173 -127T287 -150Z"></path>
<path stroke-width="1" id="E1-MJMATHI-69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path>
<path stroke-width="1" id="E1-MJMAIN-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path>
<path stroke-width="1" id="E1-MJSZ2-2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path>
<path stroke-width="1" id="E1-MJMATHI-6B" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path>
<path stroke-width="1" id="E1-MJMAIN-2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path>
<path stroke-width="1" id="E1-MJMAIN-77" d="M90 368Q84 378 76 380T40 385H18V431H24L43 430Q62 430 84 429T116 428Q206 428 221 431H229V385H215Q177 383 177 368Q177 367 221 239L265 113L339 328L333 345Q323 374 316 379Q308 384 278 385H258V431H264Q270 428 348 428Q439 428 454 431H461V385H452Q404 385 404 369Q404 366 418 324T449 234T481 143L496 100L537 219Q579 341 579 347Q579 363 564 373T530 385H522V431H529Q541 428 624 428Q692 428 698 431H703V385H697Q696 385 691 385T682 384Q635 377 619 334L559 161Q546 124 528 71Q508 12 503 1T487 -11H479Q460 -11 456 -4Q455 -3 407 133L361 267Q359 263 266 -4Q261 -11 243 -11H238Q225 -11 220 -3L90 368Z"></path>
<path stroke-width="1" id="E1-MJMAIN-6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z"></path>
<path stroke-width="1" id="E1-MJMAIN-72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z"></path>
<path stroke-width="1" id="E1-MJMAIN-6B" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T97 124T98 167T98 217T98 272T98 329Q98 366 98 407T98 482T98 542T97 586T97 603Q94 622 83 628T38 637H20V660Q20 683 22 683L32 684Q42 685 61 686T98 688Q115 689 135 690T165 693T176 694H179V463L180 233L240 287Q300 341 304 347Q310 356 310 364Q310 383 289 385H284V431H293Q308 428 412 428Q475 428 484 431H489V385H476Q407 380 360 341Q286 278 286 274Q286 273 349 181T420 79Q434 60 451 53T500 46H511V0H505Q496 3 418 3Q322 3 307 0H299V46H306Q330 48 330 65Q330 72 326 79Q323 84 276 153T228 222L176 176V120V84Q176 65 178 59T189 49Q210 46 238 46H254V0H246Q231 3 137 3T28 0H20V46H36Z"></path>
<path stroke-width="1" id="E1-MJMAIN-65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z"></path>
<path stroke-width="1" id="E1-MJMAIN-73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z"></path>
<path stroke-width="1" id="E1-MJMATHI-6A" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path>
<path stroke-width="1" id="E1-MJMAIN-47" d="M56 342Q56 428 89 500T174 615T283 681T391 705Q394 705 400 705T408 704Q499 704 569 636L582 624L612 663Q639 700 643 704Q644 704 647 704T653 705H657Q660 705 666 699V419L660 413H626Q620 419 619 430Q610 512 571 572T476 651Q457 658 426 658Q401 658 376 654T316 633T254 592T205 519T177 411Q173 369 173 335Q173 259 192 201T238 111T302 58T370 31T431 24Q478 24 513 45T559 100Q562 110 562 160V212Q561 213 557 216T551 220T542 223T526 225T502 226T463 227H437V273H449L609 270Q715 270 727 273H735V227H721Q674 227 668 215Q666 211 666 108V6Q660 0 657 0Q653 0 639 10Q617 25 600 42L587 54Q571 27 524 3T406 -22Q317 -22 238 22T108 151T56 342Z"></path>
<path stroke-width="1" id="E1-MJMAIN-50" d="M130 622Q123 629 119 631T103 634T60 637H27V683H214Q237 683 276 683T331 684Q419 684 471 671T567 616Q624 563 624 489Q624 421 573 372T451 307Q429 302 328 301H234V181Q234 62 237 58Q245 47 304 46H337V0H326Q305 3 182 3Q47 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM507 488Q507 514 506 528T500 564T483 597T450 620T397 635Q385 637 307 637H286Q237 637 234 628Q231 624 231 483V342H302H339Q390 342 423 349T481 382Q507 411 507 488Z"></path>
<path stroke-width="1" id="E1-MJMAIN-55" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q57 680 180 680Q315 680 324 683H335V637H302Q262 636 251 634T233 622L232 418V291Q232 189 240 145T280 67Q325 24 389 24Q454 24 506 64T571 183Q575 206 575 410V598Q569 608 565 613T541 627T489 637H472V683H481Q496 680 598 680T715 683H724V637H707Q634 633 622 598L621 399Q620 194 617 180Q617 179 615 171Q595 83 531 31T389 -22Q304 -22 226 33T130 192Q129 201 128 412V622Z"></path>
<path stroke-width="1" id="E1-MJMAIN-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMAINB-67" x="0" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMATHI-69" x="813" y="-336"></use>
 <use xlink:href="#E1-MJMAIN-3D" x="1197" y="0"></use>
<g transform="translate(2253,0)">
 <use xlink:href="#E1-MJSZ2-2211" x="891" y="0"></use>
<g transform="translate(0,-1110)">
 <use transform="scale(0.707)" xlink:href="#E1-MJMATHI-6B" x="0" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMAIN-2208" x="521" y="0"></use>
<g transform="translate(840,0)">
 <use transform="scale(0.707)" xlink:href="#E1-MJMAIN-77"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMAIN-6F" x="722" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMAIN-72" x="1223" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMAIN-6B" x="1615" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMAIN-65" x="2144" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMAIN-72" x="2588" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMAIN-73" x="2981" y="0"></use>
</g>
</g>
</g>
<g transform="translate(5648,0)">
 <use xlink:href="#E1-MJSZ2-2211" x="583" y="0"></use>
<g transform="translate(0,-1117)">
 <use transform="scale(0.707)" xlink:href="#E1-MJMATHI-6A" x="0" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMAIN-2208" x="412" y="0"></use>
<g transform="translate(763,0)">
 <use transform="scale(0.707)" xlink:href="#E1-MJMAIN-47"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMAIN-50" x="785" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMAIN-55" x="1467" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMAIN-73" x="2217" y="0"></use>
</g>
</g>
</g>
<g transform="translate(8425,0)">
 <use xlink:href="#E1-MJMAINB-67" x="0" y="0"></use>
<g transform="translate(575,-238)">
 <use transform="scale(0.707)" xlink:href="#E1-MJMATHI-69" x="0" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMATHI-6A" x="345" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMATHI-6B" x="758" y="0"></use>
</g>
</g>
 <use xlink:href="#E1-MJMAIN-2C" x="10005" y="0"></use>
</g>
</svg></p>
<p><em>push</em>（用于累积梯度）和 <em>pull</em>（用于取得聚合梯度）<br>“键－值存储”的 push 与 pull 操作描述如下：</p>
<ul>
<li><strong>push（key，value）</strong> 将特定的梯度值从工作节点发送到公共存储，在那里通过某种方式（例如，相加）来聚合值。</li>
<li><strong>pull（key，value）</strong> 从公共存储中取得某种方式（例如，组合来自所有工作节点的梯度）的聚合值。</li>
</ul>
<h3 id="12-7-5-小结"><a href="#12-7-5-小结" class="headerlink" title="12.7.5 小结"></a>12.7.5 小结</h3><ul>
<li>同步需要高度适应特定的网络基础设施和服务器内的连接，这种适应会严重影响同步所需的时间。</li>
<li>环同步对于 p3 和 DGX-2 服务器是最佳的，而对于其他服务器则未必。</li>
<li>当添加多个参数服务器以增加带宽时，分层同步策略可以工作的很好。</li>
</ul>

      </section>
      <section class="extra">
        
          <ul class="copyright">
  
    <li><strong>本文作者：</strong>Mengyuan Chen</li>
    <li><strong>本文链接：</strong><a href="http://example.com/2021/10/06/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%80%A7%E8%83%BD/index.html" title="http:&#x2F;&#x2F;example.com&#x2F;2021&#x2F;10&#x2F;06&#x2F;%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&#x2F;%E8%AE%A1%E7%AE%97%E6%80%A7%E8%83%BD&#x2F;index.html">http:&#x2F;&#x2F;example.com&#x2F;2021&#x2F;10&#x2F;06&#x2F;%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&#x2F;%E8%AE%A1%E7%AE%97%E6%80%A7%E8%83%BD&#x2F;index.html</a></li>
    <li><strong>版权声明：</strong>本博客所有文章均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" title="BY-NC-SA" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请注明出处！</li>
  
</ul>
        
        
          <section class="donate">
  <div id="qrcode-donate">
    <img   class="lazyload" data-original="https://sm.ms/image/Y6TiL7UgNHm2RSl" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" >
  </div>
  <div class="icon">
    <a href="javascript:;" id="alipay"><i class="iconfont iconalipay"></i></a>
    <a href="javascript:;" id="wechat"><i class="iconfont iconwechat-fill"></i></a>
  </div>
</section>
        
        
        
  <nav class="nav">
    <a href="/2021/10/06/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"><i class="iconfont iconleft"></i>第十三章 计算机视觉</a>
    <a href="/2021/10/06/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/">第十一章 优化算法<i class="iconfont iconright"></i></a>
  </nav>

      </section>
      
    </section>
  </div>
</article></div>
      <div class="col-xl-3">
        
          
  <aside class="toc-wrap">
    <h3 class="toc-title">文章目录：</h3>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%80%A7%E8%83%BD"><span class="toc-text">计算性能</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#12-1-%E7%BC%96%E8%AF%91%E5%99%A8%E5%92%8C%E8%A7%A3%E9%87%8A%E5%99%A8"><span class="toc-text">12.1 编译器和解释器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-1-1-%E7%AC%A6%E5%8F%B7%E5%BC%8F%E7%BC%96%E7%A8%8B"><span class="toc-text">12.1.1 符号式编程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-1-2-%E6%B7%B7%E5%90%88%E5%BC%8F%E7%BC%96%E7%A8%8B"><span class="toc-text">12.1.2 混合式编程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-1-3-Sequential%E7%9A%84%E6%B7%B7%E5%90%88%E5%BC%8F%E7%BC%96%E7%A8%8B"><span class="toc-text">12.1.3 Sequential的混合式编程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-1-4-%E5%B0%8F%E7%BB%93"><span class="toc-text">12.1.4 小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-3-%E5%BC%82%E6%AD%A5%E8%AE%A1%E7%AE%97"><span class="toc-text">12.3 异步计算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-2-1-%E9%80%9A%E8%BF%87%E5%90%8E%E7%AB%AF%E5%BC%82%E6%AD%A5%E5%A4%84%E7%90%86"><span class="toc-text">12.2.1 通过后端异步处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-2-2-%E9%9A%9C%E7%A2%8D%E5%99%A8%E4%B8%8E%E9%98%BB%E5%A1%9E%E5%99%A8"><span class="toc-text">12.2.2 障碍器与阻塞器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-2-3-%E6%94%B9%E8%BF%9B%E8%AE%A1%E7%AE%97"><span class="toc-text">12.2.3 改进计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-2-4-%E5%B0%8F%E7%BB%93"><span class="toc-text">12.2.4 小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-3-%E8%87%AA%E5%8A%A8%E5%B9%B6%E8%A1%8C"><span class="toc-text">12.3 自动并行</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-3-1-%E5%9F%BA%E4%BA%8EGPU%E7%9A%84%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97"><span class="toc-text">12.3.1 基于GPU的并行计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-3-2-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E4%B8%8E%E9%80%9A%E4%BF%A1"><span class="toc-text">12.3.2 并行计算与通信</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-3-3-%E5%B0%8F%E7%BB%93"><span class="toc-text">12.3.3 小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-4-%E7%A1%AC%E4%BB%B6"><span class="toc-text">12.4 硬件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-4-1-%E8%AE%A1%E7%AE%97%E6%9C%BA"><span class="toc-text">12.4.1 计算机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-4-2-%E5%86%85%E5%AD%98"><span class="toc-text">12.4.2 内存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-4-3-%E5%AD%98%E5%82%A8%E5%99%A8"><span class="toc-text">12.4.3 存储器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-4-4-CPU"><span class="toc-text">12.4.4 CPU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-4-5-GPU%E5%92%8C%E5%85%B6%E4%BB%96%E5%8A%A0%E9%80%9F%E5%8D%A1"><span class="toc-text">12.4.5 GPU和其他加速卡</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-4-6-%E7%BD%91%E7%BB%9C%E5%92%8C%E6%80%BB%E7%BA%BF"><span class="toc-text">12.4.6 网络和总线</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-4-7-%E6%9B%B4%E5%A4%9A%E5%BB%B6%E8%BF%9F"><span class="toc-text">12.4.7 更多延迟</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-4-8-%E5%B0%8F%E7%BB%93"><span class="toc-text">12.4.8 小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-5-%E5%A4%9AGPU%E8%AE%AD%E7%BB%83"><span class="toc-text">12.5 多GPU训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-5-1-%E9%97%AE%E9%A2%98%E6%8B%86%E5%88%86"><span class="toc-text">12.5.1 问题拆分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-5-2-%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E6%80%A7"><span class="toc-text">12.5.2 数据并行性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-5-3-%E7%AE%80%E5%8D%95%E7%BD%91%E7%BB%9C"><span class="toc-text">12.5.3 简单网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-5-4-%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5"><span class="toc-text">12.5.4 数据同步</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-5-5-%E6%95%B0%E6%8D%AE%E5%88%86%E5%8F%91"><span class="toc-text">12.5.5 数据分发</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-5-6-%E8%AE%AD%E7%BB%83"><span class="toc-text">12.5.6 训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-5-7-%E5%B0%8F%E7%BB%93"><span class="toc-text">12.5.7 小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-6-%E5%A4%9AGPU%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">12.6 多GPU的简洁实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-6-1-%E7%AE%80%E5%8D%95%E7%BD%91%E7%BB%9C"><span class="toc-text">12.6.1 简单网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-2-2-%E7%BD%91%E7%BB%9C%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">12.2.2 网络初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-6-3-%E8%AE%AD%E7%BB%83"><span class="toc-text">12.6.3 训练*</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-6-4-%E5%B0%8F%E7%BB%93"><span class="toc-text">12.6.4 小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-7-%E5%8F%82%E6%95%B0%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="toc-text">12.7 参数服务器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-7-1-%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83"><span class="toc-text">12.7.1 数据并行训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-7-2-%E7%8E%AF%E5%90%8C%E6%AD%A5%EF%BC%88Ring-Synchronization%EF%BC%89"><span class="toc-text">12.7.2 环同步（Ring Synchronization）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-7-3-%E5%A4%9A%E6%9C%BA%E8%AE%AD%E7%BB%83"><span class="toc-text">12.7.3 多机训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-7-4-%E9%94%AE%E5%80%BC%E5%AD%98%E5%82%A8"><span class="toc-text">12.7.4 键值存储</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-7-5-%E5%B0%8F%E7%BB%93"><span class="toc-text">12.7.5 小结</span></a></li></ol></li></ol></li></ol>
  </aside>

        
      </div>
    </div>
  </div>
</main>
  

<footer class="footer">
  <div class="footer-social"><a 
        href="tencent://message/?Menu=yes&uin=2274849184 "
        target="_blank"
        class="footer-social-item"
        onMouseOver="this.style.color= '#12B7F5'" 
        onMouseOut="this.style.color='#33333D'">
          <i class="iconfont  iconQQ "></i>
      </a><a 
        href="javascript:; "
        target="_blank"
        class="footer-social-item"
        onMouseOver="this.style.color= '#09BB07'" 
        onMouseOut="this.style.color='#33333D'">
          <i class="iconfont  iconwechat-fill "></i>
      </a><a 
        href="https://www.instagram.com/xxdsh/ "
        target="_blank"
        class="footer-social-item"
        onMouseOver="this.style.color= '#DA2E76'" 
        onMouseOut="this.style.color='#33333D'">
          <i class="iconfont  iconinstagram "></i>
      </a><a 
        href="https://github.com/xxdsh "
        target="_blank"
        class="footer-social-item"
        onMouseOver="this.style.color= '#9f7be1'" 
        onMouseOut="this.style.color='#33333D'">
          <i class="iconfont  icongithub-fill "></i>
      </a><a 
        href="mailto:mychen@buaa.edu.cn "
        target="_blank"
        class="footer-social-item"
        onMouseOver="this.style.color=#FF3B00" 
        onMouseOut="this.style.color='#33333D'">
          <i class="iconfont  iconmail"></i>
      </a></div>
  
    <div class="footer-copyright"><p>Powered by <a target="_blank" href="https://hexo.io">Hexo</a>  |  Cure The World </p></div>
  
</footer>
  
      <div class="fab fab-plus">
    <i class="iconfont iconplus"></i>
  </div>
  
  
  <div class="fab fab-up">
    <i class="iconfont iconcaret-up"></i>
  </div>
  
  
  
    
<script src="/js/color-mode.js"></script>

  
  
    <div class="search">
  <div class="search-container">
    <div class="search-close">
      <i class="iconfont iconbaseline-close-px"></i>
    </div>
    <div class="search-input-wrapper">
      <i class="search-input-icon iconfont iconsearch"></i>
      <input class="search-input" type="search" id="search-input" placeholder="Search..." autofocus autocomplete="off"
        autocorrect="off" autocapitalize="off">
    </div>
    <div class="search-output" id="search-output"></div>
  </div>
</div>
  
</body>

<script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>



  
<script src="https://cdn.bootcdn.net/ajax/libs/jquery.lazyload/1.9.1/jquery.lazyload.min.js"></script>




  
<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>






  
<script src="https://cdn.bootcdn.net/ajax/libs/jquery.qrcode/1.0/jquery.qrcode.min.js"></script>




<script src="/js/utils.js"></script>
<script src="/js/script.js"></script>







  <script>
    (function () {
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      } else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>













</html>